{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:47.041480Z",
     "iopub.status.busy": "2020-10-28T21:43:47.040602Z",
     "iopub.status.idle": "2020-10-28T21:43:52.776271Z",
     "shell.execute_reply": "2020-10-28T21:43:52.774654Z"
    },
    "papermill": {
     "duration": 5.766168,
     "end_time": "2020-10-28T21:43:52.776444",
     "exception": false,
     "start_time": "2020-10-28T21:43:47.010276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.utils import  plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import os\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')\n",
    "    print(os.getcwd())\n",
    "    os.chdir(\"/content/drive/MyDrive/CorsoForecastingEnergeticoAvanzato\")\n",
    "    print(os.getcwd())\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:52.860420Z",
     "iopub.status.busy": "2020-10-28T21:43:52.859428Z",
     "iopub.status.idle": "2020-10-28T21:43:53.205379Z",
     "shell.execute_reply": "2020-10-28T21:43:53.207050Z"
    },
    "papermill": {
     "duration": 0.393601,
     "end_time": "2020-10-28T21:43:53.207263",
     "exception": false,
     "start_time": "2020-10-28T21:43:52.813662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def load_data(col=None, path=\"/kaggle/input/energy-consumption-generation-prices-and-weather/energy_dataset.csv\", verbose=False):\n",
    "def load_data(col=None, path=\"./data/SpainPrice/energy_dataset.csv\", verbose=False):\n",
    "    df = pd.read_csv(path)\n",
    "    if col is not None:\n",
    "        df = df[col]\n",
    "    if verbose:\n",
    "        print(df.head())\n",
    "    return df\n",
    "\n",
    "print(\"Multivariate Sample\")\n",
    "multivar_df = load_data(['time', 'price actual','total load actual'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "df = load_data(col=[\"price day ahead\",\"price actual\"])\n",
    "y_true = df.iloc[:,1]\n",
    "y_pred_forecast = df.iloc[:,0]\n",
    "\n",
    "baseline_rmse = mean_squared_error(y_true, y_pred_forecast,squared=False)\n",
    "\n",
    "print(f\"\\nAverage RMSE in EUR/MWh for TSO Forecast \",baseline_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:24*31].plot(figsize=(33,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "df_load = load_data(col=[\"total load forecast\",\"total load actual\"]).dropna(axis=0)\n",
    "\n",
    "y_true = df_load.iloc[:,1]\n",
    "y_pred_forecast = df_load.iloc[:,0]\n",
    "\n",
    "baseline_rmse = mean_squared_error(y_true,y_pred_forecast,squared=False)\n",
    "print(f\"\\nAverage error in EUR/MWh for TSO Forecast \",baseline_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:24*31].plot(figsize=(33,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:24*31].plot(figsize=(33,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:56.430046Z",
     "iopub.status.busy": "2020-10-28T21:43:56.428949Z",
     "iopub.status.idle": "2020-10-28T21:43:56.433062Z",
     "shell.execute_reply": "2020-10-28T21:43:56.432356Z"
    },
    "papermill": {
     "duration": 0.041722,
     "end_time": "2020-10-28T21:43:56.433197",
     "exception": false,
     "start_time": "2020-10-28T21:43:56.391475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_data(series):\n",
    "    \"\"\"Fills missing values. \n",
    "    \n",
    "        Interpolate missing values with a linear approximation.\n",
    "    \"\"\"\n",
    "    series_filled = series.interpolate(method='linear')\n",
    "        \n",
    "    return series_filled\n",
    "        \n",
    "    \n",
    "def min_max_scale(dataframe):\n",
    "    \"\"\" Applies MinMax Scaling\n",
    "    \n",
    "        Wrapper for sklearn's MinMaxScaler class.\n",
    "    \"\"\"\n",
    "    mm = MinMaxScaler((-1,1))\n",
    "    return mm.fit_transform(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:56.537767Z",
     "iopub.status.busy": "2020-10-28T21:43:56.503720Z",
     "iopub.status.idle": "2020-10-28T21:43:56.557717Z",
     "shell.execute_reply": "2020-10-28T21:43:56.558288Z"
    },
    "papermill": {
     "duration": 0.094097,
     "end_time": "2020-10-28T21:43:56.558451",
     "exception": false,
     "start_time": "2020-10-28T21:43:56.464354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_datetime_features(series):\n",
    "    \n",
    "    #convert series to datetimes\n",
    "    times = series.apply(lambda x: x.split('+')[0])\n",
    "    datetimes = pd.DatetimeIndex(times)\n",
    "    \n",
    "    hours = datetimes.hour.values\n",
    "    day = datetimes.dayofweek.values\n",
    "    months = datetimes.month.values\n",
    "    \n",
    "    hour = pd.Series(hours, name='hours')\n",
    "    dayofw = pd.Series(day, name='dayofw')\n",
    "    month = pd.Series(months, name='months')\n",
    "    \n",
    "    return hour, dayofw, month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hour, day, month = make_datetime_features(multivar_df.time)\n",
    "print(\"Hours\")\n",
    "print(hour.head())\n",
    "print(\"Day of Week\")\n",
    "print(day.head())\n",
    "print(\"Months\")\n",
    "print(month.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:56.628190Z",
     "iopub.status.busy": "2020-10-28T21:43:56.627045Z",
     "iopub.status.idle": "2020-10-28T21:43:56.694748Z",
     "shell.execute_reply": "2020-10-28T21:43:56.695338Z"
    },
    "papermill": {
     "duration": 0.109048,
     "end_time": "2020-10-28T21:43:56.695501",
     "exception": false,
     "start_time": "2020-10-28T21:43:56.586453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(series, train_fraq, test_len=24*265):\n",
    "    \"\"\"Splits input series into train, val and test.\n",
    "    \n",
    "        Default to 1 year of test data.\n",
    "    \"\"\"\n",
    "    #slice the last year of data for testing 1 year has 8760 hours\n",
    "    test_slice = len(series)-test_len\n",
    "\n",
    "    test_data = series[test_slice:]\n",
    "    train_val_data = series[:test_slice]\n",
    "    \n",
    "    #make train and validation from the remaining\n",
    "    train_size = int(len(train_val_data) * train_fraq)\n",
    "    val_size   = int(len(train_val_data) * (1-train_fraq))\n",
    "    \n",
    "    train_data = train_val_data[:train_size]\n",
    "    val_data   = train_val_data \n",
    "    \n",
    "    print(\"train_data.shape = \", train_data.shape)\n",
    "    print(\"val_data.shape   = \", val_data.shape)\n",
    "    print(\"test_data.shape  = \", test_data.shape)\n",
    "    \n",
    "    \n",
    "    return train_data, train_val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivar_df = clean_data(multivar_df)\n",
    "\n",
    "#add hour and month features\n",
    "hours, day, months = make_datetime_features(multivar_df.time)\n",
    "multivar_df = pd.concat([multivar_df.drop(['time'], axis=1), hours, day, months], axis=1)\n",
    "multivar_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#scale\n",
    "multivar_df = min_max_scale(multivar_df)\n",
    "\n",
    "train_multi, val_multi, test_multi = split_data(multivar_df, train_fraq=0.65, test_len=8760)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Multivarate Datasets\")\n",
    "print(f\"Train Data Shape: {train_multi.shape}\")\n",
    "print(f\"Val Data Shape: {val_multi.shape}\")\n",
    "print(f\"Test Data Shape: {test_multi.shape}\")\n",
    "print(f\"Nulls In Train {np.any(np.isnan(train_multi))}\")\n",
    "print(f\"Nulls In Validation {np.any(np.isnan(val_multi))}\")\n",
    "print(f\"Nulls In Test {np.any(np.isnan(test_multi))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESEMPIO\n",
    "\n",
    "n_steps = 3\n",
    "n_horizon = 2\n",
    "batch_size = 2\n",
    "shuffle_buffer = 100\n",
    "multi_var=True \n",
    "expand_dims=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(30).reshape((-1,2))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = n_steps + n_horizon\n",
    "\n",
    "if expand_dims:\n",
    "    data = tf.expand_dims(data, axis=-1)\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices(data)\n",
    "ds = ds.window(window, shift=n_horizon, drop_remainder=True)\n",
    "ds = ds.flat_map(lambda x : x.batch(window))\n",
    "ds = ds.map(lambda x : (x[:-n_horizon], x[-n_horizon:, :1]))\n",
    "ds = ds.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Xk,Yk in ds:\n",
    "    print()\n",
    "    print(\"Xk=\",Xk)\n",
    "    print(\"-\"*50)\n",
    "    print(\"Yk=\",Yk)\n",
    "    print(\"X\"*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:56.826032Z",
     "iopub.status.busy": "2020-10-28T21:43:56.825118Z",
     "iopub.status.idle": "2020-10-28T21:43:57.065802Z",
     "shell.execute_reply": "2020-10-28T21:43:57.066564Z"
    },
    "papermill": {
     "duration": 0.286687,
     "end_time": "2020-10-28T21:43:57.066778",
     "exception": false,
     "start_time": "2020-10-28T21:43:56.780091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def window_dataset(data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=False, expand_dims=False):\n",
    "    \"\"\" \n",
    "    Create a windowed tensorflow dataset\n",
    "    \"\"\"\n",
    "\n",
    "    #create a window with n steps back plus the size of the prediction length\n",
    "    window = n_steps + n_horizon\n",
    "    \n",
    "    #expand dimensions to 3D to fit with LSTM inputs\n",
    "    #creat the inital tensor dataset\n",
    "    if expand_dims:\n",
    "        data = tf.expand_dims(data, axis=-1)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(data)\n",
    "    \n",
    "    #create the window function shifting the data by the prediction length\n",
    "    ds = ds.window(window, shift=n_horizon, drop_remainder=True)\n",
    "    \n",
    "    #flatten the dataset and batch into the window size\n",
    "    ds = ds.flat_map(lambda x : x.batch(window))\n",
    "    ds = ds.shuffle(shuffle_buffer)    \n",
    "    \n",
    "    #create the supervised learning problem x and y and batch\n",
    "    if multi_var:\n",
    "        ds = ds.map(lambda x : (x[:-n_horizon], x[-n_horizon:, :1]))\n",
    "    else:\n",
    "        ds = ds.map(lambda x : (x[:-n_horizon], x[-n_horizon:]))\n",
    "    \n",
    "    ds = ds.batch(batch_size).prefetch(32)\n",
    "    \n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "n_steps = 72\n",
    "n_horizon = 24\n",
    "batch_size = 2\n",
    "shuffle_buffer = 1000\n",
    "\n",
    "\n",
    "ds = window_dataset(train_multi, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=True)\n",
    "\n",
    "print('Example sample shapes')\n",
    "for idx,(x,y) in enumerate(ds):\n",
    "    print(\"x = \", x.numpy().shape)\n",
    "    print(\"y = \", y.numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x[0,:,0])\n",
    "plt.plot(np.arange(72,72+24),y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Example sample shapes')\n",
    "for idx,(x,y) in enumerate(ds):\n",
    "\n",
    "    plt.figure(figsize=(26,3))\n",
    "    plt.plot(x[0,:,0])\n",
    "    plt.plot(np.arange(72,72+24),y[0])\n",
    "    plt.grid()\n",
    "\n",
    "    if idx==10:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:57.204823Z",
     "iopub.status.busy": "2020-10-28T21:43:57.203757Z",
     "iopub.status.idle": "2020-10-28T21:43:57.477881Z",
     "shell.execute_reply": "2020-10-28T21:43:57.479119Z"
    },
    "papermill": {
     "duration": 0.323839,
     "end_time": "2020-10-28T21:43:57.479279",
     "exception": false,
     "start_time": "2020-10-28T21:43:57.155440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dataset(train_fraq=0.8, \n",
    "                  n_steps=24*30, \n",
    "                  n_horizon=24, \n",
    "                  batch_size=BATCH_SIZE, \n",
    "                  shuffle_buffer=24*20, \n",
    "                  expand_dims=False, \n",
    "                  multi_var=False,\n",
    "                  test_len=365*24,\n",
    "                  ):\n",
    "    \"\"\"\n",
    "    If multi variate then first column is always the column from which the target is contstructed.\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.random.set_seed(23)\n",
    "    \n",
    "    if multi_var:\n",
    "        data = load_data(col=['time', 'price actual', 'total load actual'])\n",
    "        hours, day, months = make_datetime_features(data.time)\n",
    "        data = pd.concat([data.drop(['time'], axis=1), hours, day, months], axis=1)\n",
    "    else:\n",
    "        data = load_data(col=['price actual'])\n",
    "    \n",
    "    data = clean_data(data)\n",
    "    \n",
    "    y_scaler = MinMaxScaler((-1,1))\n",
    "    scaler   = MinMaxScaler((-1,1))\n",
    "    \n",
    "    y_scaler.fit(data.iloc[:,0:1])\n",
    "    data = scaler.fit_transform(data)\n",
    "    \n",
    "    train_data, val_data, test_data = split_data(data, train_fraq=train_fraq, test_len=test_len)\n",
    "    \n",
    "    train_ds = window_dataset(train_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multi_var, expand_dims=expand_dims)\n",
    "    val_ds   = window_dataset(val_data, n_steps,  n_horizon,  batch_size, shuffle_buffer, multi_var=multi_var, expand_dims=expand_dims)\n",
    "    test_ds  = window_dataset(test_data, n_steps, n_horizon,  batch_size, shuffle_buffer, multi_var=multi_var, expand_dims=expand_dims)\n",
    "    \n",
    "    print(f\"Prediction lookback (n_steps): {n_steps}\")\n",
    "    print(f\"Prediction horizon (n_horizon): {n_horizon}\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    print(\"Datasets:\")\n",
    "    print(train_ds.element_spec)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds, y_scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds, val_ds, test_ds, y_scaler = build_dataset(multi_var=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:57.610369Z",
     "iopub.status.busy": "2020-10-28T21:43:57.609393Z",
     "iopub.status.idle": "2020-10-28T21:43:57.612573Z",
     "shell.execute_reply": "2020-10-28T21:43:57.613111Z"
    },
    "papermill": {
     "duration": 0.045045,
     "end_time": "2020-10-28T21:43:57.613253",
     "exception": false,
     "start_time": "2020-10-28T21:43:57.568208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_params(multivar=False):\n",
    "    lr = 1e-4\n",
    "    n_steps=24*30\n",
    "    n_horizon=24\n",
    "    if multivar:\n",
    "        n_features=5\n",
    "    else:\n",
    "        n_features=1\n",
    "        \n",
    "    return n_steps, n_horizon, n_features, lr\n",
    "\n",
    "def cfg_model_run(model, history, test_ds, val_ds, y_scaler):\n",
    "    return {\"model\": model, \"history\" : history, \"test_ds\": test_ds, \"val_ds\" : val_ds, \"y_scaler\" : y_scaler}\n",
    "\n",
    "\n",
    "def run_model(model_name, model_func, model_configs, epochs):\n",
    "    \n",
    "    n_steps, n_horizon, n_features, lr = get_params(multivar=True)\n",
    "    train_ds, val_ds, test_ds, y_scaler = build_dataset(n_steps=n_steps, n_horizon=n_horizon, multi_var=True)\n",
    "\n",
    "    model = model_func(n_steps, n_horizon, n_features, lr=lr)\n",
    "\n",
    "    model_hist = model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
    "\n",
    "    model_configs[model_name] = cfg_model_run(model, model_hist, test_ds, val_ds, y_scaler)\n",
    "    return test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-28T21:43:57.978118Z",
     "iopub.status.busy": "2020-10-28T21:43:57.977126Z",
     "iopub.status.idle": "2020-10-28T21:43:58.076456Z",
     "shell.execute_reply": "2020-10-28T21:43:58.077463Z"
    },
    "papermill": {
     "duration": 0.143364,
     "end_time": "2020-10-28T21:43:58.077647",
     "exception": false,
     "start_time": "2020-10-28T21:43:57.934283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cnn_model(n_steps, n_horizon, n_features, lr=3e-4):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv1D(16, kernel_size=3, activation='elu', input_shape=(n_steps,n_features)),\n",
    "        tf.keras.layers.MaxPooling1D(2),\n",
    "        tf.keras.layers.Conv1D(16, kernel_size=6, activation='elu'),\n",
    "        tf.keras.layers.MaxPooling1D(3),\n",
    "        tf.keras.layers.Conv1D(16, kernel_size=6, activation='elu'),\n",
    "        tf.keras.layers.MaxPooling1D(4),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(128),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(n_horizon)\n",
    "    ], name=\"CNN\")\n",
    "    \n",
    "    loss= tf.keras.losses.Huber()\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "cnn = cnn_model(*get_params(multivar=True))\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(cnn, \n",
    "           show_shapes=True,\n",
    "           show_layer_names=True,           \n",
    "           to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cnn_skip_model(n_steps, n_horizon, n_features, lr):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "   \n",
    "    inputs = tf.keras.layers.Input(shape=(n_steps,n_features), name='main')\n",
    "    \n",
    "    #Primo Ramo\n",
    "    conv1 = tf.keras.layers.Conv1D(64, kernel_size=6, activation='relu')(inputs)\n",
    "    max_pool_1 = tf.keras.layers.MaxPooling1D(2)(conv1)\n",
    "    conv2 = tf.keras.layers.Conv1D(64, kernel_size=3, activation='elu')(max_pool_1)\n",
    "    max_pool_2 = tf.keras.layers.MaxPooling1D(2)(conv2)\n",
    "    lstm_1 = tf.keras.layers.LSTM(64, activation='relu', return_sequences=True)(max_pool_2)\n",
    "    lstm_2 = tf.keras.layers.LSTM(64, activation='relu', return_sequences=False)(lstm_1)\n",
    "    flatten = tf.keras.layers.Flatten()(lstm_2)\n",
    "    \n",
    "    #Secondo ramo: skip\n",
    "    skip_flatten = tf.keras.layers.Flatten()(inputs)\n",
    "    dense_skip = tf.keras.layers.Dense(64, activation='relu')(skip_flatten)\n",
    "\n",
    "    #Concateno i due rami\n",
    "    concat = tf.keras.layers.Concatenate(axis=-1)([flatten, dense_skip])\n",
    "    drop_1 = tf.keras.layers.Dropout(0.5)(concat)\n",
    "    dense_1 = tf.keras.layers.Dense(128, activation='relu')(drop_1)\n",
    "    drop_2 = tf.keras.layers.Dropout(0.5)(dense_1)\n",
    "    output = tf.keras.layers.Dense(n_horizon)(drop_2)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output, name='lstm_skip')\n",
    "    \n",
    "    loss = tf.keras.losses.Huber()\n",
    "    \n",
    "    model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "lstm_skip = lstm_cnn_skip_model(*get_params(multivar=True))\n",
    "lstm_skip.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(lstm_skip, \n",
    "           show_shapes=True,\n",
    "           show_layer_names=True,           \n",
    "           to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T21:44:00.012574Z",
     "iopub.status.busy": "2020-10-28T21:44:00.011564Z",
     "iopub.status.idle": "2020-10-28T22:19:33.964140Z",
     "shell.execute_reply": "2020-10-28T22:19:33.963601Z"
    },
    "papermill": {
     "duration": 2133.999074,
     "end_time": "2020-10-28T22:19:33.964255",
     "exception": false,
     "start_time": "2020-10-28T21:43:59.965181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_configs=dict()\n",
    "run_model(\"cnn\", cnn_model, model_configs, epochs=600)\n",
    "run_model(\"lstm_skip\", lstm_cnn_skip_model, model_configs, epochs=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T22:19:38.504323Z",
     "iopub.status.busy": "2020-10-28T22:19:38.503423Z",
     "iopub.status.idle": "2020-10-28T22:19:39.313307Z",
     "shell.execute_reply": "2020-10-28T22:19:39.312557Z"
    },
    "papermill": {
     "duration": 2.101218,
     "end_time": "2020-10-28T22:19:39.313428",
     "exception": false,
     "start_time": "2020-10-28T22:19:37.212210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "legend = list()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(25,5))\n",
    "\n",
    "def plot_graphs(metric, val, ax, upper):\n",
    "    ax.plot(val['history'].history[metric])\n",
    "    ax.plot(val['history'].history[f'val_{metric}'])\n",
    "    ax.set_title(key)\n",
    "    ax.legend([metric, f\"val_{metric}\"])\n",
    "    ax.set_xlabel('epochs')\n",
    "    ax.set_ylabel(metric)\n",
    "    # ax.set_ylim([0, upper])\n",
    "    \n",
    "for (key, val), ax in zip(model_configs.items(), axs.flatten()):\n",
    "    plot_graphs('loss', val, ax, 0.2)\n",
    "print(\"Loss Curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T22:19:41.598979Z",
     "iopub.status.busy": "2020-10-28T22:19:41.597929Z",
     "iopub.status.idle": "2020-10-28T22:19:42.325267Z",
     "shell.execute_reply": "2020-10-28T22:19:42.325838Z"
    },
    "papermill": {
     "duration": 1.858989,
     "end_time": "2020-10-28T22:19:42.325984",
     "exception": false,
     "start_time": "2020-10-28T22:19:40.466995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"MAE Curves\")\n",
    "fig, axs = plt.subplots(1, 2, figsize=(25,5))\n",
    "for (key, val), ax in zip(model_configs.items(), axs.flatten()):\n",
    "    plot_graphs('mae', val, ax, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaler = model_configs['cnn']['y_scaler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-10-28T22:19:53.359148Z",
     "iopub.status.busy": "2020-10-28T22:19:53.357840Z",
     "iopub.status.idle": "2020-10-28T22:19:57.995535Z",
     "shell.execute_reply": "2020-10-28T22:19:57.996048Z"
    },
    "papermill": {
     "duration": 5.757425,
     "end_time": "2020-10-28T22:19:57.996184",
     "exception": false,
     "start_time": "2020-10-28T22:19:52.238759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(24, 12))\n",
    "days = BATCH_SIZE\n",
    "\n",
    "vline = np.linspace(0, days*24, days+1)\n",
    "\n",
    "for (key, val), ax in zip(model_configs.items(), axs):\n",
    "\n",
    "    test = val['test_ds']\n",
    "\n",
    "    xbatch, ybatch = iter(test).get_next()\n",
    "    preds = val['model'].predict(xbatch)\n",
    "\n",
    "    ybatch = ybatch.numpy()[:days].reshape(-1,1)\n",
    "    preds  = preds[:days].reshape(-1,1)\n",
    "    \n",
    "    ybatch = y_scaler.inverse_transform(ybatch)\n",
    "    preds = y_scaler.inverse_transform(preds)\n",
    "    \n",
    "    rmse = mean_squared_error(ybatch,preds,squared=False)\n",
    "    print('RMSE:',rmse)\n",
    "\n",
    "    ax.plot(ybatch)\n",
    "    ax.plot(preds)\n",
    "    ax.set_xlim(0,days*24)\n",
    "    ax.set_title(key + '  =>  RMSE=' + str(rmse))\n",
    "    ax.vlines(vline, ymin=0, ymax=1, linestyle='dotted', transform = ax.get_xaxis_transform())\n",
    "    ax.legend([\"Actual\", \"Predicted\"])\n",
    "\n",
    "plt.xlabel(\"Hours Cumulative\")\n",
    "print('First Two Weeks of Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(24, 12))\n",
    "days = 7*6\n",
    "\n",
    "vline = np.linspace(0, days*24, days+1)\n",
    "\n",
    "for (key, val), ax in zip(model_configs.items(), axs):\n",
    "\n",
    "    test = val['test_ds']\n",
    "\n",
    "    xbatch, ybatch = iter(test).get_next()\n",
    "    preds = val['model'].predict(xbatch)\n",
    "\n",
    "    ybatch = ybatch.numpy()[:days].reshape(-1,1)\n",
    "    preds  = preds[:days].reshape(-1,1)\n",
    "    \n",
    "    ybatch = y_scaler.inverse_transform(ybatch)\n",
    "    preds = y_scaler.inverse_transform(preds)\n",
    "    \n",
    "    rmse = mean_squared_error(ybatch,preds,squared=False)\n",
    "    print('RMSE:',rmse)\n",
    "\n",
    "    ax.plot(ybatch)\n",
    "    ax.plot(preds)\n",
    "    ax.set_xlim(0,days*24)\n",
    "    ax.set_title(key + '  =>  RMSE=' + str(rmse))\n",
    "    ax.vlines(vline, ymin=0, ymax=1, linestyle='dotted', transform = ax.get_xaxis_transform())\n",
    "    ax.legend([\"Actual\", \"Predicted\"])\n",
    "\n",
    "plt.xlabel(\"Hours Cumulative\")\n",
    "print('First Two Weeks of Predictions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "papermill": {
   "duration": 2181.254074,
   "end_time": "2020-10-28T22:20:03.921952",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-28T21:43:42.667878",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
